---
title: "Next Word Prediction with Smoothing Technique"
description: "implementation of Kneseer-Ney Smoothing Technique for next word prediction in sequence."
image: "images/word_predeiction.png"
date : 08/28/2024
categories : ["Statistical Modelling","Code","Python"]

# Code Properties
execute: 
  cache: true
  enabled: true
code-line-numbers: true
code-overflow: scroll
code-copy: true
---
 
![](images/word_predeiction.png)
<p style="text-align: center;">Image From Internet</p>
<br><br>

The Other day i was wokring on one of the Natural Language Module and I stumbled on one of the technique, which is Kneseer-Ney Smoothing Technique for finding next word in sequence. Let't not get into much details. The equation was bit complex to understand, so found some resouces and understood the equation well, but when I was searching for implementation in python , I couldn't find, so  I thought to implement it.
 

### Importing Libraries

I will be using `ngram `from `nltk` library to create a 
```{python}
import string
from nltk.util import ngrams
from collections import OrderedDict, defaultdict
from datetime import datetime
from tqdm import tqdm
```

### Loading Data
```{python}

def loadCorpus(file_path, bi_dict, tri_dict, quad_dict, vocab_dict):
    token = []
    word_len = 0

    with open(file_path,'r') as file:
        lines  = [ x.strip() for x in file.readlines()]
    lines = ['<start> '+x+' <end>' for x in lines]
    for line in lines:
        temp_l = line.split()
        # print(temp_l)
        i = 0
        j = 0
        
        for word in temp_l :
            j = 0
            for l in word :
                if l in '!"#$%&\'()*+,-./:;=?@[\\]^_`{|}~':
                    if l == "'":
                        if j+1<len(word) and word[j+1] == 's':
                            j = j + 1
                            continue
                    word = word.replace(l," ")
                    #print(j,word[j])
                j += 1

            temp_l[i] = word.lower()
            i=i+1   

        content = " ".join(temp_l)

        token = content.split()
        word_len = word_len + len(token)  

        if not token:
            continue

        temp0 = list(ngrams(token,2))
       
        temp1 = list(ngrams(token,3))

        for word in token:
            if word not in vocab_dict:
                vocab_dict[word] = 1
            else:
                vocab_dict[word]+= 1
                
        temp2 = list(ngrams(token,4))

        for t in temp0:
            sen = ' '.join(t)
            bi_dict[sen] += 1

        for t in temp1:
            sen = ' '.join(t)
            tri_dict[sen] += 1

        for t in temp2:
            sen = ' '.join(t)
            quad_dict[sen] += 1

        n = len(token)
           
    return word_len
```

### Generating Kneeser-Ney Dictonaries
```{python}

def createKNDict(ngram_dict, n):

    i = 0
    d = 0.75

    first_dict = {}
    
    sec_dict = {}
    
    for key in ngram_dict:
        
        ngram_token = key.split()
       
        n_1gram_sen = ' '.join(ngram_token[:n-1])
         
        if n_1gram_sen not in sec_dict:
            sec_dict[ n_1gram_sen ] = 1
        else:
            sec_dict[ n_1gram_sen ] += 1
            
        if ngram_token[-1] not in first_dict:
            first_dict[ ngram_token[-1] ] = 1
        else:
            first_dict[ ngram_token[-1] ] += 1
    
    return first_dict, sec_dict
```

### Creating Kneeser-Ney Probablities Dictonary

```{python}
def computeKnesserNeyProbablity(vocab_dict, ngram_dicts, prob_dict):
    d = 0.75
    interpolation = 0.4  # Adjust as needed

    for order in range(2, len(ngram_dicts) + 2):
        current_dict = ngram_dicts[order - 2]

        first_dict, sec_dict = createKNDict(current_dict, order)

        for ngram in tqdm(current_dict):
            ngram_tokens = ngram.split()
            prefix = ' '.join(ngram_tokens[:-1])

            prob1 = max(current_dict[ngram] - d, 0) / sec_dict[prefix] if prefix in sec_dict else 0
            prob2 = d / sec_dict[prefix] * (first_dict[ngram_tokens[-1]] if ngram_tokens[-1] in first_dict else 0)

            for i in range(order - 2, 0, -1):
                ngram_prefix = ' '.join(ngram_tokens[i:-1])
                prob2 *= d / len(ngram_dicts[i - 1]) * (sec_dict[ngram_prefix] if ngram_prefix in sec_dict else 0)

            prob_dict[prefix] = prob_dict.get(prefix, [])
            prob_dict[prefix].append([(1 - interpolation) * (prob1 + prob2) + interpolation * vocab_dict[ngram_tokens[-1]] / sum(vocab_dict.values()), ngram_tokens[-1]])

    print("Completed")
```
### Utilites Function

```{python}
def sortProbWordDict(prob_dict):
    for key in prob_dict:
        if len(prob_dict[key])>0:
            prob_dict[key] = sorted(prob_dict[key],reverse = True)[:2]


def removePunctuations(sen):
    """
    Funtion to remove punctuations from the given input sentence and covert them to lowercase.
    arg: string
    returns: string
    """
    temp_l = sen.split()
    i = 0
    j = 0
    
    for word in temp_l :
        j = 0
        #print(len(word))
        for l in word :
            if l in string.punctuation:
                if l == "'":
                    if j+1<len(word) and word[j+1] == 's':
                        j = j + 1
                        continue
                word = word.replace(l," ")
            j += 1

        temp_l[i] = word.lower()
        i=i+1   
    content = " ".join(temp_l)
    return content
def doPrediction(sen, prob_dict):
    if sen in prob_dict:
        return prob_dict[sen]
    else:
        return ""
```
### Start of The Code
The variable  `bi_dict`, `tri_dict`, `quad_dict`, `prob_dict`

```{python}

bi_dict = defaultdict(int)
tri_dict = defaultdict(int)            
quad_dict = defaultdict(int)   
vocab_dict = defaultdict(int)       
prob_dict = OrderedDict()         

quad_dict = defaultdict(int)   

token_len = loadCorpus("last.txt",bi_dict,tri_dict,quad_dict,vocab_dict)

computeKnesserNeyProbablity(vocab_dict, [bi_dict, tri_dict, quad_dict] ,prob_dict )
sortProbWordDict(prob_dict)

```
### Predction Function  

The function `get_words()` takes the text from the user and, check if given string is Empty, is more then three characthers.

Adds a `<start>` tag to the input, marking it as start of the input.
The  function 

```{python}
def get_words(text):
    inp_time = datetime.now()
    if text.split() == [] and len(text.split())>0:
        print("Input Text Found to be Empty.")
    text = removePunctuations(text)
    text = "<start> "+text
    if len(text.split())>3:
        text = text.split()
        text = " ".join(text[-3:])
        
    final_words = doPrediction(text.lower(),prob_dict)

    print('Word Prediction:',final_words)
    inp_proc_time = datetime.now()
    print('----------------------------Prediction Time :',inp_proc_time-inp_time)
```

### Generating Response 1
Resposne for "Hi".
```{python}
get_words("Hi")
```

### Generating Response 2
Response for  "Good Morning".
```{python}
get_words("good Morning")
```

### Generating Response 3 
Let's check the output for  "What are".
```{python}
get_words("What are")
```



