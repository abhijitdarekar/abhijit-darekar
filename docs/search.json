[
  {
    "objectID": "pages/beam_search.html",
    "href": "pages/beam_search.html",
    "title": "Beam Search in LSTM",
    "section": "",
    "text": "Beam Search in LSTM"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Abhijit Darekar",
    "section": "",
    "text": "Test Messa"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Abhijit Darekar",
    "section": "",
    "text": "About Me\nLorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\n\n\nTimeline\n\n\n\n\n\n\n\n\nRole\nCompany\nDuration\n\n\n\n\nMachine Learning Engineer\nSkycliff IT\nJan 2024 - Present\n\n\nPython Developer\nTata Consultancy Service\nSep 2020 - Nov 2022"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Work done by me as a part of Learning and Expirementaions.\n\n\n\n\n\n\n\n\n\n\n\n\nBeam Search in LSTM\n\n\nSample Text to Display here\n\n\nWhen reading the contents of a listing, Quarto uses the metadata read from the front matter of the document or the contents of the document itself to populate the following fields for each item:\n\n\n\n\n\nAug 28, 2024\n\n\nAbhijit Darekar\n\n\n\n\n\n\n\n\n\n\n\n\nNext Word Prediction\n\n\nSample Text to Display here\n\n\nWhen reading the contents of a listing, Quarto uses the metadata read from the front matter of the document or the contents of the document itself to populate the following fields for each item\n\n\n\n\n\nAug 28, 2024\n\n\nAbhijit Darekar\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "pages/nsp.html",
    "href": "pages/nsp.html",
    "title": "Next Word Prediction",
    "section": "",
    "text": "Image From Internet\n\n Lorem ipsum dolor sit amet, consectetur adipiscing elit. Proin sodales dolor risus, laoreet pulvinar est tincidunt ac. Suspendisse eu nisl orci. Pellentesque lobortis sagittis massa eget tempor. Proin porttitor efficitur ligula, ut venenatis mauris. Praesent venenatis commodo leo a accumsan. Vivamus vestibulum velit rhoncus, tempus augue non, sollicitudin nulla. Curabitur vitae velit sed sapien molestie efficitur non nec orci. Vestibulum ante ipsum primis in faucibus orci luctus et ultrices posuere cubilia curae; Curabitur in porttitor tortor. Donec sit amet leo vitae velit rhoncus malesuada. Duis aliquam, sapien ut luctus convallis, magna nisl maximus ante, at dignissim elit magna sed mauris. Nunc ac ex velit.\nwefwfm\n\nImporting Libraries\n\nimport string\nfrom nltk.util import ngrams\nfrom collections import OrderedDict, defaultdict, namedtuple\nfrom datetime import datetime\nfrom tqdm import tqdm\n\n\n\nLoading Data\n\nfirst = True\ndef loadCorpus(file_path, bi_dict, tri_dict, quad_dict, vocab_dict):\n    token = []\n    word_len = 0\n\n    with open(file_path,'r') as file:\n        lines  = [ x.strip() for x in file.readlines()]\n    lines = ['&lt;start&gt; '+x+' &lt;end&gt;' for x in lines]\n    for line in lines:\n        temp_l = line.split()\n        # print(temp_l)\n        i = 0\n        j = 0\n        \n        for word in temp_l :\n            j = 0\n            for l in word :\n                if l in '!\"#$%&\\'()*+,-./:;=?@[\\\\]^_`{|}~':\n                    if l == \"'\":\n                        if j+1&lt;len(word) and word[j+1] == 's':\n                            j = j + 1\n                            continue\n                    word = word.replace(l,\" \")\n                    #print(j,word[j])\n                j += 1\n\n            temp_l[i] = word.lower()\n            i=i+1   \n\n        content = \" \".join(temp_l)\n\n        token = content.split()\n        word_len = word_len + len(token)  \n\n        if not token:\n            continue\n\n        temp0 = list(ngrams(token,2))\n       \n        temp1 = list(ngrams(token,3))\n\n        for word in token:\n            if word not in vocab_dict:\n                vocab_dict[word] = 1\n            else:\n                vocab_dict[word]+= 1\n                \n        temp2 = list(ngrams(token,4))\n\n        for t in temp0:\n            sen = ' '.join(t)\n            bi_dict[sen] += 1\n\n        for t in temp1:\n            sen = ' '.join(t)\n            tri_dict[sen] += 1\n\n        for t in temp2:\n            sen = ' '.join(t)\n            quad_dict[sen] += 1\n\n        n = len(token)\n           \n    return word_len\n\n\n\nCreating Kneseer-Net Dict\n\nfirst = True\ndef createKNDict(ngram_dict, n):\n\n    i = 0\n    d = 0.75\n\n    first_dict = {}\n    \n    sec_dict = {}\n    \n    for key in ngram_dict:\n        \n        ngram_token = key.split()\n       \n        n_1gram_sen = ' '.join(ngram_token[:n-1])\n         \n        if n_1gram_sen not in sec_dict:\n            sec_dict[ n_1gram_sen ] = 1\n        else:\n            sec_dict[ n_1gram_sen ] += 1\n            \n        if ngram_token[-1] not in first_dict:\n            first_dict[ ngram_token[-1] ] = 1\n        else:\n            first_dict[ ngram_token[-1] ] += 1\n    \n    return first_dict, sec_dict\n\n\nfirst = True\ndef computeKnesserNeyProb(vocab_dict, bi_dict, tri_dict, quad_dict, prob_dict ):\n        d = 0.75\n       \n        quad_first_dict, quad_sec_dict = createKNDict(quad_dict, 4)\n    \n        tri_first_dict, tri_sec_dict = createKNDict(tri_dict, 3)\n        \n        bi_first_dict, bi_sec_dict = createKNDict(bi_dict, 2)\n        # For Higher order Ngram range 4     \n        for quad in quad_dict:\n            quad_token = quad.split()\n         \n            tri_sen = ' '.join(quad_token[:-1])\n           \n            quad_prob1 = max( quad_dict[quad] - d, 0) / tri_dict[tri_sen]\n            quad_prob2 = d/tri_dict[tri_sen] * ( quad_sec_dict[tri_sen] )\n            \n            tri_prob1 = max( tri_first_dict[quad_token[-1]] - d, 0) / len(tri_dict)\n            tri_prob2 = ( d/len(tri_dict) )* ( tri_sec_dict[' '.join(quad_token[1:3])] )\n            \n            bi_prob1 = max( bi_first_dict[quad_token[-1]] - d, 0) / len(bi_dict)\n            bi_prob2 = ( d/len(bi_dict) ) * ( bi_sec_dict[quad_token[-2]])\n            uni_prob = bi_first_dict[quad_token[-1]] / len(bi_dict)\n            \n            prob = quad_prob1 + quad_prob2*( tri_prob1 + tri_prob2*( bi_prob1 + bi_prob2* uni_prob ) )\n           \n            if tri_sen not in prob_dict:\n                prob_dict[tri_sen] = []\n                prob_dict[tri_sen].append([prob,quad_token[-1]])\n            else:\n                prob_dict[tri_sen].append([prob,quad_token[-1]])\n        #for ngram range 3\n        for tri in tri_dict:\n            tri_token = tri.split()\n\n            bi_sen = \" \".join(tri_token[:-1])\n            tri_prob1 = max(tri_dict[tri] -d , 0) / bi_dict[bi_sen]\n            tri_prob2 =  (d/bi_dict[bi_sen]) * (tri_sec_dict[bi_sen])\n\n            bi_prob1 = max(bi_first_dict[tri_token[-1]] -d,0)/len(bi_dict)\n            bi_prob2 = (d/len(bi_dict)) * (bi_sec_dict[tri_token[-2]])\n\n            uni_prob = bi_first_dict[tri_token[-1]]/len(bi_dict)\n            prob = tri_prob1 + tri_prob2*( bi_prob1 + bi_prob2* uni_prob )\n            if bi_sen not in prob_dict:\n                prob_dict[bi_sen] = []\n                prob_dict[bi_sen].append([prob,tri_token[-1]])\n            else:\n                prob_dict[bi_sen].append([prob,tri_token[-1]])\n        \n        #for ngram range 2        \n        for bi in bi_dict:\n            bi_token = bi.split() \n            sen = \" \".join(bi_token[:-1])\n\n            bi_prob1 = max(bi_dict[bi] - d,0)/vocab_dict[sen]\n            bi_prob2 = (d/vocab_dict[sen]) *( bi_sec_dict[bi_token[-2]])\n            \n            uni_prob = bi_first_dict[bi_token[-1]]/len(bi_dict)\n            \n            prob = bi_prob1 + bi_prob2* uni_prob\n\n            if sen not in prob_dict:\n                prob_dict[sen] = []\n                prob_dict[sen].append([prob,bi_token[-1]])\n            else:\n                prob_dict[sen].append([prob,bi_token[-1]])\n        \n        print(\"Completed\")\n        \ndef sortProbWordDict(prob_dict):\n    for key in prob_dict:\n        if len(prob_dict[key])&gt;0:\n            prob_dict[key] = sorted(prob_dict[key],reverse = True)[:2]\n\n\nfirst = True\ndef removePunctuations(sen):\n    \"\"\"\n    Funtion to remove punctuations from the given input sentence and covert them to lowercase.\n    arg: string\n    returns: string\n    \"\"\"\n    temp_l = sen.split()\n    i = 0\n    j = 0\n    \n    for word in temp_l :\n        j = 0\n        #print(len(word))\n        for l in word :\n            if l in string.punctuation:\n                if l == \"'\":\n                    if j+1&lt;len(word) and word[j+1] == 's':\n                        j = j + 1\n                        continue\n                word = word.replace(l,\" \")\n            j += 1\n\n        temp_l[i] = word.lower()\n        i=i+1   \n    content = \" \".join(temp_l)\n    return content\ndef doPrediction(sen, prob_dict):\n    if sen in prob_dict:\n        return prob_dict[sen]\n    else:\n        return \"\"\n\n\ndef computeKnesserNeyProb2(vocab_dict, ngram_dicts, prob_dict):\n    d = 0.75\n    interpolation = 0.4  # Adjust as needed\n\n    for order in range(2, len(ngram_dicts) + 2):\n        current_dict = ngram_dicts[order - 2]\n\n        first_dict, sec_dict = createKNDict(current_dict, order)\n\n        for ngram in tqdm(current_dict):\n            ngram_tokens = ngram.split()\n            prefix = ' '.join(ngram_tokens[:-1])\n\n            prob1 = max(current_dict[ngram] - d, 0) / sec_dict[prefix] if prefix in sec_dict else 0\n            prob2 = d / sec_dict[prefix] * (first_dict[ngram_tokens[-1]] if ngram_tokens[-1] in first_dict else 0)\n\n            for i in range(order - 2, 0, -1):\n                ngram_prefix = ' '.join(ngram_tokens[i:-1])\n                prob2 *= d / len(ngram_dicts[i - 1]) * (sec_dict[ngram_prefix] if ngram_prefix in sec_dict else 0)\n\n            prob_dict[prefix] = prob_dict.get(prefix, [])\n            prob_dict[prefix].append([(1 - interpolation) * (prob1 + prob2) + interpolation * vocab_dict[ngram_tokens[-1]] / sum(vocab_dict.values()), ngram_tokens[-1]])\n\n    print(\"Completed\")\n\n\nif first:\n    bi_dict = defaultdict(int)\n    tri_dict = defaultdict(int)            \n    quad_dict = defaultdict(int)   \n    vocab_dict = defaultdict(int)       \n    prob_dict = OrderedDict()         \n\n    quad_dict = defaultdict(int)   \n\n    token_len = loadCorpus(\"last.txt\",bi_dict,tri_dict,quad_dict,vocab_dict)\n\n    computeKnesserNeyProb2(vocab_dict, [bi_dict, tri_dict, quad_dict] ,prob_dict )\n    sortProbWordDict(prob_dict)\n    first = False\n    \ndef get_words(text):\n    inp_time = datetime.now()\n    if text.split() == [] and len(text.split())&gt;0:\n        print(\"Input Text Found to be Empty.\")\n    text = removePunctuations(text)\n    text = \"&lt;start&gt; \"+text\n    if len(text.split())&gt;3:\n        text = text.split()\n        text = \" \".join(text[-3:])\n        \n    final_words = doPrediction(text.lower(),prob_dict)\n\n    print('Word Prediction:',final_words)\n    inp_proc_time = datetime.now()\n    print('----------------------------Prediction Time :',inp_proc_time-inp_time)\n\n  0%|          | 0/9845 [00:00&lt;?, ?it/s] 80%|███████▉  | 7843/9845 [00:00&lt;00:00, 78421.10it/s]100%|██████████| 9845/9845 [00:00&lt;00:00, 76856.07it/s]\n  0%|          | 0/19829 [00:00&lt;?, ?it/s] 38%|███▊      | 7521/19829 [00:00&lt;00:00, 75206.31it/s] 76%|███████▌  | 15042/19829 [00:00&lt;00:00, 71884.22it/s]100%|██████████| 19829/19829 [00:00&lt;00:00, 72218.26it/s]\n  0%|          | 0/23911 [00:00&lt;?, ?it/s] 30%|██▉       | 7095/23911 [00:00&lt;00:00, 70945.84it/s] 59%|█████▉    | 14190/23911 [00:00&lt;00:00, 56280.67it/s] 87%|████████▋ | 20920/23911 [00:00&lt;00:00, 60683.61it/s]100%|██████████| 23911/23911 [00:00&lt;00:00, 61314.78it/s]\n\n\nCompleted\n\n\n\n\n\n\nget_words(\"Hi\")\n\nWord Prediction: [[0.9761837911670966, 'how'], [0.6773237382168934, 'what']]\n----------------------------Prediction Time : 0:00:00.000136\n\n\n\nget_words(\"good Morning\")\n\nWord Prediction: [[0.15004384411729987, 'team']]\n----------------------------Prediction Time : 0:00:00.000226\n\n\n\nget_words(\"What are\")\n\nWord Prediction: [[1.719622097435119, 'you'], [0.5116390954621338, 'we']]\n----------------------------Prediction Time : 0:00:00.000132"
  }
]