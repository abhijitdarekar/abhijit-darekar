[
  {
    "objectID": "pages/Smoothing Technique - Kneser Ney.html",
    "href": "pages/Smoothing Technique - Kneser Ney.html",
    "title": "Abhijit Darekar",
    "section": "",
    "text": "import string\nfrom nltk.util import ngrams\nfrom collections import OrderedDict, defaultdict, namedtuple\nfrom datetime import datetime\nfrom tqdm import tqdm\n\n\nfirst = True\ndef loadCorpus(file_path, bi_dict, tri_dict, quad_dict, vocab_dict):\n    token = []\n    word_len = 0\n\n    with open(file_path,'r') as file:\n        lines  = [ x.strip() for x in file.readlines()]\n    lines = ['&lt;start&gt; '+x+' &lt;end&gt;' for x in lines]\n    for line in lines:\n        temp_l = line.split()\n        # print(temp_l)\n        i = 0\n        j = 0\n        \n        for word in temp_l :\n            j = 0\n            for l in word :\n                if l in '!\"#$%&\\'()*+,-./:;=?@[\\\\]^_`{|}~':\n                    if l == \"'\":\n                        if j+1&lt;len(word) and word[j+1] == 's':\n                            j = j + 1\n                            continue\n                    word = word.replace(l,\" \")\n                    #print(j,word[j])\n                j += 1\n\n            temp_l[i] = word.lower()\n            i=i+1   \n\n        content = \" \".join(temp_l)\n\n        token = content.split()\n        word_len = word_len + len(token)  \n\n        if not token:\n            continue\n\n        temp0 = list(ngrams(token,2))\n       \n        temp1 = list(ngrams(token,3))\n\n        for word in token:\n            if word not in vocab_dict:\n                vocab_dict[word] = 1\n            else:\n                vocab_dict[word]+= 1\n                \n        temp2 = list(ngrams(token,4))\n\n        for t in temp0:\n            sen = ' '.join(t)\n            bi_dict[sen] += 1\n\n        for t in temp1:\n            sen = ' '.join(t)\n            tri_dict[sen] += 1\n\n        for t in temp2:\n            sen = ' '.join(t)\n            quad_dict[sen] += 1\n\n        n = len(token)\n           \n    return word_len\n\n\nfirst = True\ndef createKNDict(ngram_dict, n):\n\n    i = 0\n    d = 0.75\n\n    first_dict = {}\n    \n    sec_dict = {}\n    \n    for key in ngram_dict:\n        \n        ngram_token = key.split()\n       \n        n_1gram_sen = ' '.join(ngram_token[:n-1])\n         \n        if n_1gram_sen not in sec_dict:\n            sec_dict[ n_1gram_sen ] = 1\n        else:\n            sec_dict[ n_1gram_sen ] += 1\n            \n        if ngram_token[-1] not in first_dict:\n            first_dict[ ngram_token[-1] ] = 1\n        else:\n            first_dict[ ngram_token[-1] ] += 1\n    \n    return first_dict, sec_dict\n\n\nfirst = True\ndef computeKnesserNeyProb(vocab_dict, bi_dict, tri_dict, quad_dict, prob_dict ):\n        d = 0.75\n       \n        quad_first_dict, quad_sec_dict = createKNDict(quad_dict, 4)\n    \n        tri_first_dict, tri_sec_dict = createKNDict(tri_dict, 3)\n        \n        bi_first_dict, bi_sec_dict = createKNDict(bi_dict, 2)\n        # For Higher order Ngram range 4     \n        for quad in quad_dict:\n            quad_token = quad.split()\n         \n            tri_sen = ' '.join(quad_token[:-1])\n           \n            quad_prob1 = max( quad_dict[quad] - d, 0) / tri_dict[tri_sen]\n            quad_prob2 = d/tri_dict[tri_sen] * ( quad_sec_dict[tri_sen] )\n            \n            tri_prob1 = max( tri_first_dict[quad_token[-1]] - d, 0) / len(tri_dict)\n            tri_prob2 = ( d/len(tri_dict) )* ( tri_sec_dict[' '.join(quad_token[1:3])] )\n            \n            bi_prob1 = max( bi_first_dict[quad_token[-1]] - d, 0) / len(bi_dict)\n            bi_prob2 = ( d/len(bi_dict) ) * ( bi_sec_dict[quad_token[-2]])\n            uni_prob = bi_first_dict[quad_token[-1]] / len(bi_dict)\n            \n            prob = quad_prob1 + quad_prob2*( tri_prob1 + tri_prob2*( bi_prob1 + bi_prob2* uni_prob ) )\n           \n            if tri_sen not in prob_dict:\n                prob_dict[tri_sen] = []\n                prob_dict[tri_sen].append([prob,quad_token[-1]])\n            else:\n                prob_dict[tri_sen].append([prob,quad_token[-1]])\n        #for ngram range 3\n        for tri in tri_dict:\n            tri_token = tri.split()\n\n            bi_sen = \" \".join(tri_token[:-1])\n            tri_prob1 = max(tri_dict[tri] -d , 0) / bi_dict[bi_sen]\n            tri_prob2 =  (d/bi_dict[bi_sen]) * (tri_sec_dict[bi_sen])\n\n            bi_prob1 = max(bi_first_dict[tri_token[-1]] -d,0)/len(bi_dict)\n            bi_prob2 = (d/len(bi_dict)) * (bi_sec_dict[tri_token[-2]])\n\n            uni_prob = bi_first_dict[tri_token[-1]]/len(bi_dict)\n            prob = tri_prob1 + tri_prob2*( bi_prob1 + bi_prob2* uni_prob )\n            if bi_sen not in prob_dict:\n                prob_dict[bi_sen] = []\n                prob_dict[bi_sen].append([prob,tri_token[-1]])\n            else:\n                prob_dict[bi_sen].append([prob,tri_token[-1]])\n        \n        #for ngram range 2        \n        for bi in bi_dict:\n            bi_token = bi.split() \n            sen = \" \".join(bi_token[:-1])\n\n            bi_prob1 = max(bi_dict[bi] - d,0)/vocab_dict[sen]\n            bi_prob2 = (d/vocab_dict[sen]) *( bi_sec_dict[bi_token[-2]])\n            \n            uni_prob = bi_first_dict[bi_token[-1]]/len(bi_dict)\n            \n            prob = bi_prob1 + bi_prob2* uni_prob\n\n            if sen not in prob_dict:\n                prob_dict[sen] = []\n                prob_dict[sen].append([prob,bi_token[-1]])\n            else:\n                prob_dict[sen].append([prob,bi_token[-1]])\n        \n        print(\"Completed\")\n        \ndef sortProbWordDict(prob_dict):\n    for key in prob_dict:\n        if len(prob_dict[key])&gt;0:\n            prob_dict[key] = sorted(prob_dict[key],reverse = True)[:2]\n\n\nfirst = True\ndef removePunctuations(sen):\n    \"\"\"\n    Funtion to remove punctuations from the given input sentence and covert them to lowercase.\n    arg: string\n    returns: string\n    \"\"\"\n    temp_l = sen.split()\n    i = 0\n    j = 0\n    \n    for word in temp_l :\n        j = 0\n        #print(len(word))\n        for l in word :\n            if l in string.punctuation:\n                if l == \"'\":\n                    if j+1&lt;len(word) and word[j+1] == 's':\n                        j = j + 1\n                        continue\n                word = word.replace(l,\" \")\n            j += 1\n\n        temp_l[i] = word.lower()\n        i=i+1   \n    content = \" \".join(temp_l)\n    return content\ndef doPrediction(sen, prob_dict):\n    if sen in prob_dict:\n        return prob_dict[sen]\n    else:\n        return \"\"\n\n\ndef computeKnesserNeyProb2(vocab_dict, ngram_dicts, prob_dict):\n    d = 0.75\n    interpolation = 0.4  # Adjust as needed\n\n    for order in range(2, len(ngram_dicts) + 2):\n        current_dict = ngram_dicts[order - 2]\n\n        first_dict, sec_dict = createKNDict(current_dict, order)\n\n        for ngram in tqdm(current_dict):\n            ngram_tokens = ngram.split()\n            prefix = ' '.join(ngram_tokens[:-1])\n\n            prob1 = max(current_dict[ngram] - d, 0) / sec_dict[prefix] if prefix in sec_dict else 0\n            prob2 = d / sec_dict[prefix] * (first_dict[ngram_tokens[-1]] if ngram_tokens[-1] in first_dict else 0)\n\n            for i in range(order - 2, 0, -1):\n                ngram_prefix = ' '.join(ngram_tokens[i:-1])\n                prob2 *= d / len(ngram_dicts[i - 1]) * (sec_dict[ngram_prefix] if ngram_prefix in sec_dict else 0)\n\n            prob_dict[prefix] = prob_dict.get(prefix, [])\n            prob_dict[prefix].append([(1 - interpolation) * (prob1 + prob2) + interpolation * vocab_dict[ngram_tokens[-1]] / sum(vocab_dict.values()), ngram_tokens[-1]])\n\n    print(\"Completed\")\n\n\nif first:\n    bi_dict = defaultdict(int)\n    tri_dict = defaultdict(int)            \n    quad_dict = defaultdict(int)   \n    vocab_dict = defaultdict(int)       \n    prob_dict = OrderedDict()         \n\n    quad_dict = defaultdict(int)   \n\n    token_len = loadCorpus(\"last.txt\",bi_dict,tri_dict,quad_dict,vocab_dict)\n\n    computeKnesserNeyProb2(vocab_dict, [bi_dict, tri_dict, quad_dict] ,prob_dict )\n    sortProbWordDict(prob_dict)\n    first = False\ndef get_prediction(text):\n    inp_time = datetime.now()\n    if text.split() == [] and len(text.split())&gt;0:\n        return None\n    if text == \"exit\": \n        return None\n    text = removePunctuations(text)\n    text = \"&lt;start&gt; \"+text\n    if len(text.split())&gt;3:\n        text = text.split()\n        text = \" \".join(text[-3:])\n        \n    final_words = doPrediction(text,prob_dict)\n\n    print('Word Prediction:',final_words)\n    inp_proc_time = datetime.now()\n    print('----------------------------Prediction Time :',inp_proc_time-inp_time)\n\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9845/9845 [00:00&lt;00:00, 52084.00it/s]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 19829/19829 [00:00&lt;00:00, 50198.40it/s]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 23911/23911 [00:00&lt;00:00, 47098.86it/s]\n\n\nCompleted\nEnter the sentence to predict : abhijit is a \nWord Prediction: \n----------------------------Prediction Time : 0:00:00\nEnter the sentence to predict : is there any \nWord Prediction: \n----------------------------Prediction Time : 0:00:00\nEnter the sentence to predict : what are tou\nWord Prediction: \n----------------------------Prediction Time : 0:00:00.001021\nEnter the sentence to predict : what are you\nWord Prediction: [[0.3190434183234684, 'doing'], [0.3124132603160149, '&lt;end&gt;']]\n----------------------------Prediction Time : 0:00:00\nEnter the sentence to predict : exit"
  },
  {
    "objectID": "pages/nsp.html",
    "href": "pages/nsp.html",
    "title": "Next Word Prediction",
    "section": "",
    "text": "import string\nfrom nltk.util import ngrams\nfrom collections import OrderedDict, defaultdict, namedtuple\nfrom datetime import datetime\nfrom tqdm import tqdm\n\n\nfirst = True\ndef loadCorpus(file_path, bi_dict, tri_dict, quad_dict, vocab_dict):\n    token = []\n    word_len = 0\n\n    with open(file_path,'r') as file:\n        lines  = [ x.strip() for x in file.readlines()]\n    lines = ['&lt;start&gt; '+x+' &lt;end&gt;' for x in lines]\n    for line in lines:\n        temp_l = line.split()\n        # print(temp_l)\n        i = 0\n        j = 0\n        \n        for word in temp_l :\n            j = 0\n            for l in word :\n                if l in '!\"#$%&\\'()*+,-./:;=?@[\\\\]^_`{|}~':\n                    if l == \"'\":\n                        if j+1&lt;len(word) and word[j+1] == 's':\n                            j = j + 1\n                            continue\n                    word = word.replace(l,\" \")\n                    #print(j,word[j])\n                j += 1\n\n            temp_l[i] = word.lower()\n            i=i+1   \n\n        content = \" \".join(temp_l)\n\n        token = content.split()\n        word_len = word_len + len(token)  \n\n        if not token:\n            continue\n\n        temp0 = list(ngrams(token,2))\n       \n        temp1 = list(ngrams(token,3))\n\n        for word in token:\n            if word not in vocab_dict:\n                vocab_dict[word] = 1\n            else:\n                vocab_dict[word]+= 1\n                \n        temp2 = list(ngrams(token,4))\n\n        for t in temp0:\n            sen = ' '.join(t)\n            bi_dict[sen] += 1\n\n        for t in temp1:\n            sen = ' '.join(t)\n            tri_dict[sen] += 1\n\n        for t in temp2:\n            sen = ' '.join(t)\n            quad_dict[sen] += 1\n\n        n = len(token)\n           \n    return word_len\n\n\nfirst = True\ndef createKNDict(ngram_dict, n):\n\n    i = 0\n    d = 0.75\n\n    first_dict = {}\n    \n    sec_dict = {}\n    \n    for key in ngram_dict:\n        \n        ngram_token = key.split()\n       \n        n_1gram_sen = ' '.join(ngram_token[:n-1])\n         \n        if n_1gram_sen not in sec_dict:\n            sec_dict[ n_1gram_sen ] = 1\n        else:\n            sec_dict[ n_1gram_sen ] += 1\n            \n        if ngram_token[-1] not in first_dict:\n            first_dict[ ngram_token[-1] ] = 1\n        else:\n            first_dict[ ngram_token[-1] ] += 1\n    \n    return first_dict, sec_dict\n\n\nfirst = True\ndef computeKnesserNeyProb(vocab_dict, bi_dict, tri_dict, quad_dict, prob_dict ):\n        d = 0.75\n       \n        quad_first_dict, quad_sec_dict = createKNDict(quad_dict, 4)\n    \n        tri_first_dict, tri_sec_dict = createKNDict(tri_dict, 3)\n        \n        bi_first_dict, bi_sec_dict = createKNDict(bi_dict, 2)\n        # For Higher order Ngram range 4     \n        for quad in quad_dict:\n            quad_token = quad.split()\n         \n            tri_sen = ' '.join(quad_token[:-1])\n           \n            quad_prob1 = max( quad_dict[quad] - d, 0) / tri_dict[tri_sen]\n            quad_prob2 = d/tri_dict[tri_sen] * ( quad_sec_dict[tri_sen] )\n            \n            tri_prob1 = max( tri_first_dict[quad_token[-1]] - d, 0) / len(tri_dict)\n            tri_prob2 = ( d/len(tri_dict) )* ( tri_sec_dict[' '.join(quad_token[1:3])] )\n            \n            bi_prob1 = max( bi_first_dict[quad_token[-1]] - d, 0) / len(bi_dict)\n            bi_prob2 = ( d/len(bi_dict) ) * ( bi_sec_dict[quad_token[-2]])\n            uni_prob = bi_first_dict[quad_token[-1]] / len(bi_dict)\n            \n            prob = quad_prob1 + quad_prob2*( tri_prob1 + tri_prob2*( bi_prob1 + bi_prob2* uni_prob ) )\n           \n            if tri_sen not in prob_dict:\n                prob_dict[tri_sen] = []\n                prob_dict[tri_sen].append([prob,quad_token[-1]])\n            else:\n                prob_dict[tri_sen].append([prob,quad_token[-1]])\n        #for ngram range 3\n        for tri in tri_dict:\n            tri_token = tri.split()\n\n            bi_sen = \" \".join(tri_token[:-1])\n            tri_prob1 = max(tri_dict[tri] -d , 0) / bi_dict[bi_sen]\n            tri_prob2 =  (d/bi_dict[bi_sen]) * (tri_sec_dict[bi_sen])\n\n            bi_prob1 = max(bi_first_dict[tri_token[-1]] -d,0)/len(bi_dict)\n            bi_prob2 = (d/len(bi_dict)) * (bi_sec_dict[tri_token[-2]])\n\n            uni_prob = bi_first_dict[tri_token[-1]]/len(bi_dict)\n            prob = tri_prob1 + tri_prob2*( bi_prob1 + bi_prob2* uni_prob )\n            if bi_sen not in prob_dict:\n                prob_dict[bi_sen] = []\n                prob_dict[bi_sen].append([prob,tri_token[-1]])\n            else:\n                prob_dict[bi_sen].append([prob,tri_token[-1]])\n        \n        #for ngram range 2        \n        for bi in bi_dict:\n            bi_token = bi.split() \n            sen = \" \".join(bi_token[:-1])\n\n            bi_prob1 = max(bi_dict[bi] - d,0)/vocab_dict[sen]\n            bi_prob2 = (d/vocab_dict[sen]) *( bi_sec_dict[bi_token[-2]])\n            \n            uni_prob = bi_first_dict[bi_token[-1]]/len(bi_dict)\n            \n            prob = bi_prob1 + bi_prob2* uni_prob\n\n            if sen not in prob_dict:\n                prob_dict[sen] = []\n                prob_dict[sen].append([prob,bi_token[-1]])\n            else:\n                prob_dict[sen].append([prob,bi_token[-1]])\n        \n        print(\"Completed\")\n        \ndef sortProbWordDict(prob_dict):\n    for key in prob_dict:\n        if len(prob_dict[key])&gt;0:\n            prob_dict[key] = sorted(prob_dict[key],reverse = True)[:2]\n\n\nfirst = True\ndef removePunctuations(sen):\n    \"\"\"\n    Funtion to remove punctuations from the given input sentence and covert them to lowercase.\n    arg: string\n    returns: string\n    \"\"\"\n    temp_l = sen.split()\n    i = 0\n    j = 0\n    \n    for word in temp_l :\n        j = 0\n        #print(len(word))\n        for l in word :\n            if l in string.punctuation:\n                if l == \"'\":\n                    if j+1&lt;len(word) and word[j+1] == 's':\n                        j = j + 1\n                        continue\n                word = word.replace(l,\" \")\n            j += 1\n\n        temp_l[i] = word.lower()\n        i=i+1   \n    content = \" \".join(temp_l)\n    return content\ndef doPrediction(sen, prob_dict):\n    if sen in prob_dict:\n        return prob_dict[sen]\n    else:\n        return \"\"\n\n\ndef computeKnesserNeyProb2(vocab_dict, ngram_dicts, prob_dict):\n    d = 0.75\n    interpolation = 0.4  # Adjust as needed\n\n    for order in range(2, len(ngram_dicts) + 2):\n        current_dict = ngram_dicts[order - 2]\n\n        first_dict, sec_dict = createKNDict(current_dict, order)\n\n        for ngram in tqdm(current_dict):\n            ngram_tokens = ngram.split()\n            prefix = ' '.join(ngram_tokens[:-1])\n\n            prob1 = max(current_dict[ngram] - d, 0) / sec_dict[prefix] if prefix in sec_dict else 0\n            prob2 = d / sec_dict[prefix] * (first_dict[ngram_tokens[-1]] if ngram_tokens[-1] in first_dict else 0)\n\n            for i in range(order - 2, 0, -1):\n                ngram_prefix = ' '.join(ngram_tokens[i:-1])\n                prob2 *= d / len(ngram_dicts[i - 1]) * (sec_dict[ngram_prefix] if ngram_prefix in sec_dict else 0)\n\n            prob_dict[prefix] = prob_dict.get(prefix, [])\n            prob_dict[prefix].append([(1 - interpolation) * (prob1 + prob2) + interpolation * vocab_dict[ngram_tokens[-1]] / sum(vocab_dict.values()), ngram_tokens[-1]])\n\n    print(\"Completed\")\n\n\nif first:\n    bi_dict = defaultdict(int)\n    tri_dict = defaultdict(int)            \n    quad_dict = defaultdict(int)   \n    vocab_dict = defaultdict(int)       \n    prob_dict = OrderedDict()         \n\n    quad_dict = defaultdict(int)   \n\n    token_len = loadCorpus(\"last.txt\",bi_dict,tri_dict,quad_dict,vocab_dict)\n\n    computeKnesserNeyProb2(vocab_dict, [bi_dict, tri_dict, quad_dict] ,prob_dict )\n    sortProbWordDict(prob_dict)\n    first = False\n    \ndef get_words(text):\n    inp_time = datetime.now()\n    if text.split() == [] and len(text.split())&gt;0:\n        print(\"Input Text Found to be Empty.\")\n    text = removePunctuations(text)\n    text = \"&lt;start&gt; \"+text\n    if len(text.split())&gt;3:\n        text = text.split()\n        text = \" \".join(text[-3:])\n        \n    final_words = doPrediction(text.lower(),prob_dict)\n\n    print('Word Prediction:',final_words)\n    inp_proc_time = datetime.now()\n    print('----------------------------Prediction Time :',inp_proc_time-inp_time)\n\n  0%|          | 0/9845 [00:00&lt;?, ?it/s] 45%|████▍     | 4384/9845 [00:00&lt;00:00, 43837.53it/s]100%|██████████| 9845/9845 [00:00&lt;00:00, 55941.10it/s]\n  0%|          | 0/19829 [00:00&lt;?, ?it/s] 30%|██▉       | 5898/19829 [00:00&lt;00:00, 58972.89it/s] 62%|██████▏   | 12393/19829 [00:00&lt;00:00, 62486.68it/s] 98%|█████████▊| 19485/19829 [00:00&lt;00:00, 66333.04it/s]100%|██████████| 19829/19829 [00:00&lt;00:00, 64694.20it/s]\n  0%|          | 0/23911 [00:00&lt;?, ?it/s] 29%|██▉       | 6917/23911 [00:00&lt;00:00, 69165.61it/s] 60%|██████    | 14440/23911 [00:00&lt;00:00, 72727.14it/s] 91%|█████████ | 21713/23911 [00:00&lt;00:00, 71914.84it/s]100%|██████████| 23911/23911 [00:00&lt;00:00, 71708.55it/s]\n\n\nCompleted\n\n\n\n\n\n\nget_words(\"Hi\")\n\nWord Prediction: [[0.9761837911670966, 'how'], [0.6773237382168934, 'what']]\n----------------------------Prediction Time : 0:00:00.000125\n\n\n\nget_words(\"good Morning\")\n\nWord Prediction: [[0.15004384411729987, 'team']]\n----------------------------Prediction Time : 0:00:00.000080\n\n\n\nget_words(\"What are\")\n\nWord Prediction: [[1.719622097435119, 'you'], [0.5116390954621338, 'we']]\n----------------------------Prediction Time : 0:00:00.000186"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Work done by me as a part of Learning and Expirementaions.\n\n\n\n\n\n\n\n\n\n\n\n\nBeam Search in LSTM\n\n\nSample Text to Display here\n\n\nWhen reading the contents of a listing, Quarto uses the metadata read from the front matter of the document or the contents of the document itself to populate the following fields for each item:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNext Word Prediction\n\n\nSample Text to Display here\n\n\nWhen reading the contents of a listing, Quarto uses the metadata read from the front matter of the document or the contents of the document itself to populate the following fields for each item:\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "test",
    "section": "",
    "text": "Test Messa"
  },
  {
    "objectID": "pages/beam_search.html",
    "href": "pages/beam_search.html",
    "title": "Beam Search in LSTM",
    "section": "",
    "text": "Beam Search in LSTM"
  }
]