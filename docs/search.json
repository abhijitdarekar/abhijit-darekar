[
  {
    "objectID": "posts/Smoothing Technique - Kneser Ney.html",
    "href": "posts/Smoothing Technique - Kneser Ney.html",
    "title": "Abhijit Darekar",
    "section": "",
    "text": "import string\nfrom nltk.util import ngrams\nfrom collections import OrderedDict, defaultdict, namedtuple\nfrom datetime import datetime\nfrom tqdm import tqdm\n\n\nfirst = True\ndef loadCorpus(file_path, bi_dict, tri_dict, quad_dict, vocab_dict):\n    token = []\n    word_len = 0\n\n    with open(file_path,'r') as file:\n        lines  = [ x.strip() for x in file.readlines()]\n    lines = ['&lt;start&gt; '+x+' &lt;end&gt;' for x in lines]\n    for line in lines:\n        temp_l = line.split()\n        # print(temp_l)\n        i = 0\n        j = 0\n        \n        for word in temp_l :\n            j = 0\n            for l in word :\n                if l in '!\"#$%&\\'()*+,-./:;=?@[\\\\]^_`{|}~':\n                    if l == \"'\":\n                        if j+1&lt;len(word) and word[j+1] == 's':\n                            j = j + 1\n                            continue\n                    word = word.replace(l,\" \")\n                    #print(j,word[j])\n                j += 1\n\n            temp_l[i] = word.lower()\n            i=i+1   \n\n        content = \" \".join(temp_l)\n\n        token = content.split()\n        word_len = word_len + len(token)  \n\n        if not token:\n            continue\n\n        temp0 = list(ngrams(token,2))\n       \n        temp1 = list(ngrams(token,3))\n\n        for word in token:\n            if word not in vocab_dict:\n                vocab_dict[word] = 1\n            else:\n                vocab_dict[word]+= 1\n                \n        temp2 = list(ngrams(token,4))\n\n        for t in temp0:\n            sen = ' '.join(t)\n            bi_dict[sen] += 1\n\n        for t in temp1:\n            sen = ' '.join(t)\n            tri_dict[sen] += 1\n\n        for t in temp2:\n            sen = ' '.join(t)\n            quad_dict[sen] += 1\n\n        n = len(token)\n           \n    return word_len\n\n\nfirst = True\ndef createKNDict(ngram_dict, n):\n\n    i = 0\n    d = 0.75\n\n    first_dict = {}\n    \n    sec_dict = {}\n    \n    for key in ngram_dict:\n        \n        ngram_token = key.split()\n       \n        n_1gram_sen = ' '.join(ngram_token[:n-1])\n         \n        if n_1gram_sen not in sec_dict:\n            sec_dict[ n_1gram_sen ] = 1\n        else:\n            sec_dict[ n_1gram_sen ] += 1\n            \n        if ngram_token[-1] not in first_dict:\n            first_dict[ ngram_token[-1] ] = 1\n        else:\n            first_dict[ ngram_token[-1] ] += 1\n    \n    return first_dict, sec_dict\n\n\nfirst = True\ndef computeKnesserNeyProb(vocab_dict, bi_dict, tri_dict, quad_dict, prob_dict ):\n        d = 0.75\n       \n        quad_first_dict, quad_sec_dict = createKNDict(quad_dict, 4)\n    \n        tri_first_dict, tri_sec_dict = createKNDict(tri_dict, 3)\n        \n        bi_first_dict, bi_sec_dict = createKNDict(bi_dict, 2)\n        # For Higher order Ngram range 4     \n        for quad in quad_dict:\n            quad_token = quad.split()\n         \n            tri_sen = ' '.join(quad_token[:-1])\n           \n            quad_prob1 = max( quad_dict[quad] - d, 0) / tri_dict[tri_sen]\n            quad_prob2 = d/tri_dict[tri_sen] * ( quad_sec_dict[tri_sen] )\n            \n            tri_prob1 = max( tri_first_dict[quad_token[-1]] - d, 0) / len(tri_dict)\n            tri_prob2 = ( d/len(tri_dict) )* ( tri_sec_dict[' '.join(quad_token[1:3])] )\n            \n            bi_prob1 = max( bi_first_dict[quad_token[-1]] - d, 0) / len(bi_dict)\n            bi_prob2 = ( d/len(bi_dict) ) * ( bi_sec_dict[quad_token[-2]])\n            uni_prob = bi_first_dict[quad_token[-1]] / len(bi_dict)\n            \n            prob = quad_prob1 + quad_prob2*( tri_prob1 + tri_prob2*( bi_prob1 + bi_prob2* uni_prob ) )\n           \n            if tri_sen not in prob_dict:\n                prob_dict[tri_sen] = []\n                prob_dict[tri_sen].append([prob,quad_token[-1]])\n            else:\n                prob_dict[tri_sen].append([prob,quad_token[-1]])\n        #for ngram range 3\n        for tri in tri_dict:\n            tri_token = tri.split()\n\n            bi_sen = \" \".join(tri_token[:-1])\n            tri_prob1 = max(tri_dict[tri] -d , 0) / bi_dict[bi_sen]\n            tri_prob2 =  (d/bi_dict[bi_sen]) * (tri_sec_dict[bi_sen])\n\n            bi_prob1 = max(bi_first_dict[tri_token[-1]] -d,0)/len(bi_dict)\n            bi_prob2 = (d/len(bi_dict)) * (bi_sec_dict[tri_token[-2]])\n\n            uni_prob = bi_first_dict[tri_token[-1]]/len(bi_dict)\n            prob = tri_prob1 + tri_prob2*( bi_prob1 + bi_prob2* uni_prob )\n            if bi_sen not in prob_dict:\n                prob_dict[bi_sen] = []\n                prob_dict[bi_sen].append([prob,tri_token[-1]])\n            else:\n                prob_dict[bi_sen].append([prob,tri_token[-1]])\n        \n        #for ngram range 2        \n        for bi in bi_dict:\n            bi_token = bi.split() \n            sen = \" \".join(bi_token[:-1])\n\n            bi_prob1 = max(bi_dict[bi] - d,0)/vocab_dict[sen]\n            bi_prob2 = (d/vocab_dict[sen]) *( bi_sec_dict[bi_token[-2]])\n            \n            uni_prob = bi_first_dict[bi_token[-1]]/len(bi_dict)\n            \n            prob = bi_prob1 + bi_prob2* uni_prob\n\n            if sen not in prob_dict:\n                prob_dict[sen] = []\n                prob_dict[sen].append([prob,bi_token[-1]])\n            else:\n                prob_dict[sen].append([prob,bi_token[-1]])\n        \n        print(\"Completed\")\n        \ndef sortProbWordDict(prob_dict):\n    for key in prob_dict:\n        if len(prob_dict[key])&gt;0:\n            prob_dict[key] = sorted(prob_dict[key],reverse = True)[:2]\n\n\nfirst = True\ndef removePunctuations(sen):\n    \"\"\"\n    Funtion to remove punctuations from the given input sentence and covert them to lowercase.\n    arg: string\n    returns: string\n    \"\"\"\n    temp_l = sen.split()\n    i = 0\n    j = 0\n    \n    for word in temp_l :\n        j = 0\n        #print(len(word))\n        for l in word :\n            if l in string.punctuation:\n                if l == \"'\":\n                    if j+1&lt;len(word) and word[j+1] == 's':\n                        j = j + 1\n                        continue\n                word = word.replace(l,\" \")\n            j += 1\n\n        temp_l[i] = word.lower()\n        i=i+1   \n    content = \" \".join(temp_l)\n    return content\ndef doPrediction(sen, prob_dict):\n    if sen in prob_dict:\n        return prob_dict[sen]\n    else:\n        return \"\"\n\n\ndef computeKnesserNeyProb2(vocab_dict, ngram_dicts, prob_dict):\n    d = 0.75\n    interpolation = 0.4  # Adjust as needed\n\n    for order in range(2, len(ngram_dicts) + 2):\n        current_dict = ngram_dicts[order - 2]\n\n        first_dict, sec_dict = createKNDict(current_dict, order)\n\n        for ngram in tqdm(current_dict):\n            ngram_tokens = ngram.split()\n            prefix = ' '.join(ngram_tokens[:-1])\n\n            prob1 = max(current_dict[ngram] - d, 0) / sec_dict[prefix] if prefix in sec_dict else 0\n            prob2 = d / sec_dict[prefix] * (first_dict[ngram_tokens[-1]] if ngram_tokens[-1] in first_dict else 0)\n\n            for i in range(order - 2, 0, -1):\n                ngram_prefix = ' '.join(ngram_tokens[i:-1])\n                prob2 *= d / len(ngram_dicts[i - 1]) * (sec_dict[ngram_prefix] if ngram_prefix in sec_dict else 0)\n\n            prob_dict[prefix] = prob_dict.get(prefix, [])\n            prob_dict[prefix].append([(1 - interpolation) * (prob1 + prob2) + interpolation * vocab_dict[ngram_tokens[-1]] / sum(vocab_dict.values()), ngram_tokens[-1]])\n\n    print(\"Completed\")\n\n\nif first:\n    bi_dict = defaultdict(int)\n    tri_dict = defaultdict(int)            \n    quad_dict = defaultdict(int)   \n    vocab_dict = defaultdict(int)       \n    prob_dict = OrderedDict()         \n\n    quad_dict = defaultdict(int)   \n\n    token_len = loadCorpus(\"last.txt\",bi_dict,tri_dict,quad_dict,vocab_dict)\n\n    computeKnesserNeyProb2(vocab_dict, [bi_dict, tri_dict, quad_dict] ,prob_dict )\n    sortProbWordDict(prob_dict)\n    first = False\n    \ndef get_words(text):\n    inp_time = datetime.now()\n    if text.split() == [] and len(text.split())&gt;0:\n        print(\"Input Text Found to be Empty.\")\n    text = removePunctuations(text)\n    text = \"&lt;start&gt; \"+text\n    if len(text.split())&gt;3:\n        text = text.split()\n        text = \" \".join(text[-3:])\n        \n    final_words = doPrediction(text.lower(),prob_dict)\n\n    print('Word Prediction:',final_words)\n    inp_proc_time = datetime.now()\n    print('----------------------------Prediction Time :',inp_proc_time-inp_time)\n\n\nget_words(\"hi\")\n\nWord Prediction: [[0.9761837911670966, 'how'], [0.6773237382168934, 'what']]\n----------------------------Prediction Time : 0:00:00\n\n\n\nget_words(\"good Morning \")\n\nWord Prediction: [[0.15004384411729987, 'team']]\n----------------------------Prediction Time : 0:00:00\n\n\n\nget_words(\"What are\")\n\nWord Prediction: [[1.719622097435119, 'you'], [0.5116390954621338, 'we']]\n----------------------------Prediction Time : 0:00:00\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/nsp.html",
    "href": "posts/nsp.html",
    "title": "Next Word Prediction with Smoothing Technique",
    "section": "",
    "text": "Image From Internet\n\n\nThe Other day i was wokring on one of the Natural Language Module and I stumbled on one of the technique, which is Kneseer-Ney Smoothing Technique for finding next word in sequence. Let’t not get into much details. The equation was bit complex to understand, so found some resouces and understood the equation well, but when I was searching for implementation in python , I couldn’t find, so I thought to implement it.\nwefwfm\n\nImporting Libraries\n\nimport string\nfrom nltk.util import ngrams\nfrom collections import OrderedDict, defaultdict, namedtuple\nfrom datetime import datetime\nfrom tqdm import tqdm\n\n\n\nLoading Data\n\nfirst = True\ndef loadCorpus(file_path, bi_dict, tri_dict, quad_dict, vocab_dict):\n    token = []\n    word_len = 0\n\n    with open(file_path,'r') as file:\n        lines  = [ x.strip() for x in file.readlines()]\n    lines = ['&lt;start&gt; '+x+' &lt;end&gt;' for x in lines]\n    for line in lines:\n        temp_l = line.split()\n        # print(temp_l)\n        i = 0\n        j = 0\n        \n        for word in temp_l :\n            j = 0\n            for l in word :\n                if l in '!\"#$%&\\'()*+,-./:;=?@[\\\\]^_`{|}~':\n                    if l == \"'\":\n                        if j+1&lt;len(word) and word[j+1] == 's':\n                            j = j + 1\n                            continue\n                    word = word.replace(l,\" \")\n                    #print(j,word[j])\n                j += 1\n\n            temp_l[i] = word.lower()\n            i=i+1   \n\n        content = \" \".join(temp_l)\n\n        token = content.split()\n        word_len = word_len + len(token)  \n\n        if not token:\n            continue\n\n        temp0 = list(ngrams(token,2))\n       \n        temp1 = list(ngrams(token,3))\n\n        for word in token:\n            if word not in vocab_dict:\n                vocab_dict[word] = 1\n            else:\n                vocab_dict[word]+= 1\n                \n        temp2 = list(ngrams(token,4))\n\n        for t in temp0:\n            sen = ' '.join(t)\n            bi_dict[sen] += 1\n\n        for t in temp1:\n            sen = ' '.join(t)\n            tri_dict[sen] += 1\n\n        for t in temp2:\n            sen = ' '.join(t)\n            quad_dict[sen] += 1\n\n        n = len(token)\n           \n    return word_len\n\n\n\nCreating Kneseer-Net Dict\n\nfirst = True\ndef createKNDict(ngram_dict, n):\n\n    i = 0\n    d = 0.75\n\n    first_dict = {}\n    \n    sec_dict = {}\n    \n    for key in ngram_dict:\n        \n        ngram_token = key.split()\n       \n        n_1gram_sen = ' '.join(ngram_token[:n-1])\n         \n        if n_1gram_sen not in sec_dict:\n            sec_dict[ n_1gram_sen ] = 1\n        else:\n            sec_dict[ n_1gram_sen ] += 1\n            \n        if ngram_token[-1] not in first_dict:\n            first_dict[ ngram_token[-1] ] = 1\n        else:\n            first_dict[ ngram_token[-1] ] += 1\n    \n    return first_dict, sec_dict\n\n\nfirst = True\ndef computeKnesserNeyProb(vocab_dict, bi_dict, tri_dict, quad_dict, prob_dict ):\n        d = 0.75\n       \n        quad_first_dict, quad_sec_dict = createKNDict(quad_dict, 4)\n    \n        tri_first_dict, tri_sec_dict = createKNDict(tri_dict, 3)\n        \n        bi_first_dict, bi_sec_dict = createKNDict(bi_dict, 2)\n        # For Higher order Ngram range 4     \n        for quad in quad_dict:\n            quad_token = quad.split()\n         \n            tri_sen = ' '.join(quad_token[:-1])\n           \n            quad_prob1 = max( quad_dict[quad] - d, 0) / tri_dict[tri_sen]\n            quad_prob2 = d/tri_dict[tri_sen] * ( quad_sec_dict[tri_sen] )\n            \n            tri_prob1 = max( tri_first_dict[quad_token[-1]] - d, 0) / len(tri_dict)\n            tri_prob2 = ( d/len(tri_dict) )* ( tri_sec_dict[' '.join(quad_token[1:3])] )\n            \n            bi_prob1 = max( bi_first_dict[quad_token[-1]] - d, 0) / len(bi_dict)\n            bi_prob2 = ( d/len(bi_dict) ) * ( bi_sec_dict[quad_token[-2]])\n            uni_prob = bi_first_dict[quad_token[-1]] / len(bi_dict)\n            \n            prob = quad_prob1 + quad_prob2*( tri_prob1 + tri_prob2*( bi_prob1 + bi_prob2* uni_prob ) )\n           \n            if tri_sen not in prob_dict:\n                prob_dict[tri_sen] = []\n                prob_dict[tri_sen].append([prob,quad_token[-1]])\n            else:\n                prob_dict[tri_sen].append([prob,quad_token[-1]])\n        #for ngram range 3\n        for tri in tri_dict:\n            tri_token = tri.split()\n\n            bi_sen = \" \".join(tri_token[:-1])\n            tri_prob1 = max(tri_dict[tri] -d , 0) / bi_dict[bi_sen]\n            tri_prob2 =  (d/bi_dict[bi_sen]) * (tri_sec_dict[bi_sen])\n\n            bi_prob1 = max(bi_first_dict[tri_token[-1]] -d,0)/len(bi_dict)\n            bi_prob2 = (d/len(bi_dict)) * (bi_sec_dict[tri_token[-2]])\n\n            uni_prob = bi_first_dict[tri_token[-1]]/len(bi_dict)\n            prob = tri_prob1 + tri_prob2*( bi_prob1 + bi_prob2* uni_prob )\n            if bi_sen not in prob_dict:\n                prob_dict[bi_sen] = []\n                prob_dict[bi_sen].append([prob,tri_token[-1]])\n            else:\n                prob_dict[bi_sen].append([prob,tri_token[-1]])\n        \n        #for ngram range 2        \n        for bi in bi_dict:\n            bi_token = bi.split() \n            sen = \" \".join(bi_token[:-1])\n\n            bi_prob1 = max(bi_dict[bi] - d,0)/vocab_dict[sen]\n            bi_prob2 = (d/vocab_dict[sen]) *( bi_sec_dict[bi_token[-2]])\n            \n            uni_prob = bi_first_dict[bi_token[-1]]/len(bi_dict)\n            \n            prob = bi_prob1 + bi_prob2* uni_prob\n\n            if sen not in prob_dict:\n                prob_dict[sen] = []\n                prob_dict[sen].append([prob,bi_token[-1]])\n            else:\n                prob_dict[sen].append([prob,bi_token[-1]])\n        \n        print(\"Completed\")\n        \ndef sortProbWordDict(prob_dict):\n    for key in prob_dict:\n        if len(prob_dict[key])&gt;0:\n            prob_dict[key] = sorted(prob_dict[key],reverse = True)[:2]\n\n\nfirst = True\ndef removePunctuations(sen):\n    \"\"\"\n    Funtion to remove punctuations from the given input sentence and covert them to lowercase.\n    arg: string\n    returns: string\n    \"\"\"\n    temp_l = sen.split()\n    i = 0\n    j = 0\n    \n    for word in temp_l :\n        j = 0\n        #print(len(word))\n        for l in word :\n            if l in string.punctuation:\n                if l == \"'\":\n                    if j+1&lt;len(word) and word[j+1] == 's':\n                        j = j + 1\n                        continue\n                word = word.replace(l,\" \")\n            j += 1\n\n        temp_l[i] = word.lower()\n        i=i+1   \n    content = \" \".join(temp_l)\n    return content\ndef doPrediction(sen, prob_dict):\n    if sen in prob_dict:\n        return prob_dict[sen]\n    else:\n        return \"\"\n\n\ndef computeKnesserNeyProb2(vocab_dict, ngram_dicts, prob_dict):\n    d = 0.75\n    interpolation = 0.4  # Adjust as needed\n\n    for order in range(2, len(ngram_dicts) + 2):\n        current_dict = ngram_dicts[order - 2]\n\n        first_dict, sec_dict = createKNDict(current_dict, order)\n\n        for ngram in tqdm(current_dict):\n            ngram_tokens = ngram.split()\n            prefix = ' '.join(ngram_tokens[:-1])\n\n            prob1 = max(current_dict[ngram] - d, 0) / sec_dict[prefix] if prefix in sec_dict else 0\n            prob2 = d / sec_dict[prefix] * (first_dict[ngram_tokens[-1]] if ngram_tokens[-1] in first_dict else 0)\n\n            for i in range(order - 2, 0, -1):\n                ngram_prefix = ' '.join(ngram_tokens[i:-1])\n                prob2 *= d / len(ngram_dicts[i - 1]) * (sec_dict[ngram_prefix] if ngram_prefix in sec_dict else 0)\n\n            prob_dict[prefix] = prob_dict.get(prefix, [])\n            prob_dict[prefix].append([(1 - interpolation) * (prob1 + prob2) + interpolation * vocab_dict[ngram_tokens[-1]] / sum(vocab_dict.values()), ngram_tokens[-1]])\n\n    print(\"Completed\")\n\n\nif first:\n    bi_dict = defaultdict(int)\n    tri_dict = defaultdict(int)            \n    quad_dict = defaultdict(int)   \n    vocab_dict = defaultdict(int)       \n    prob_dict = OrderedDict()         \n\n    quad_dict = defaultdict(int)   \n\n    token_len = loadCorpus(\"last.txt\",bi_dict,tri_dict,quad_dict,vocab_dict)\n\n    computeKnesserNeyProb2(vocab_dict, [bi_dict, tri_dict, quad_dict] ,prob_dict )\n    sortProbWordDict(prob_dict)\n    first = False\n    \ndef get_words(text):\n    inp_time = datetime.now()\n    if text.split() == [] and len(text.split())&gt;0:\n        print(\"Input Text Found to be Empty.\")\n    text = removePunctuations(text)\n    text = \"&lt;start&gt; \"+text\n    if len(text.split())&gt;3:\n        text = text.split()\n        text = \" \".join(text[-3:])\n        \n    final_words = doPrediction(text.lower(),prob_dict)\n\n    print('Word Prediction:',final_words)\n    inp_proc_time = datetime.now()\n    print('----------------------------Prediction Time :',inp_proc_time-inp_time)\n\n  0%|          | 0/9845 [00:00&lt;?, ?it/s] 50%|████▉     | 4877/9845 [00:00&lt;00:00, 48764.35it/s]100%|██████████| 9845/9845 [00:00&lt;00:00, 58770.09it/s]\n  0%|          | 0/19829 [00:00&lt;?, ?it/s] 32%|███▏      | 6373/19829 [00:00&lt;00:00, 63724.74it/s] 68%|██████▊   | 13448/19829 [00:00&lt;00:00, 67851.53it/s]100%|██████████| 19829/19829 [00:00&lt;00:00, 69421.06it/s]\n  0%|          | 0/23911 [00:00&lt;?, ?it/s] 33%|███▎      | 7818/23911 [00:00&lt;00:00, 78159.95it/s] 65%|██████▌   | 15649/23911 [00:00&lt;00:00, 78244.41it/s] 98%|█████████▊| 23474/23911 [00:00&lt;00:00, 72911.51it/s]100%|██████████| 23911/23911 [00:00&lt;00:00, 69776.47it/s]\n\n\nCompleted\n\n\n\n\n\n\nget_words(\"Hi\")\n\nWord Prediction: [[0.9761837911670966, 'how'], [0.6773237382168934, 'what']]\n----------------------------Prediction Time : 0:00:00.000096\n\n\n\nget_words(\"good Morning\")\n\nWord Prediction: [[0.15004384411729987, 'team']]\n----------------------------Prediction Time : 0:00:00.000115\n\n\n\nget_words(\"What are\")\n\nWord Prediction: [[1.719622097435119, 'you'], [0.5116390954621338, 'we']]\n----------------------------Prediction Time : 0:00:00.000095\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Work done by me as a part of Learning and Expirementaions.\n\n\n\n\n\n\n\n\n\n\n\n\nA Gentle Guide to Beam Search with LSTM\n\n\n\n\n\nA Simple Guide to Implement Beam Search on LSTM models for better text generation.\n\n\n\n\n\nAug 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nNext Word Prediction with Smoothing Technique\n\n\n\n\n\nimplementation of Kneseer-Ney Smoothing Technique for next word prediction in sequence.\n\n\n\n\n\nAug 28, 2024\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Abhijit Darekar",
    "section": "",
    "text": "I have diverse knowledge in Machine Learning and Deep Learning field. I have worked on various projects from showcasing in POC to deploying them in prodution. I have through knwoledge on ML Algorihtm, DL Archiectures, cloud Services such as Azure OpenAI, AWS SageMaker, AWS Lambda Funcions, AWS Api Route, AWS ECR & EC2 and Github Actions for CI/CD.\n      I am active in Kaggel, participating in competations, to testing out LLM(s) and searching for datasets. I have recieved Kaggle Notebooks Exprets Badge.\n\n\n\n\n\n\n\n\n\n\n\nRole\nCompany\nDuration\n\n\n\n\nMachine Learning Engineer\nSkycliff IT\nJan 2024 - Present\n\n\nPython Developer\nTata Consultancy Service\nSep 2020 - Nov 2022"
  },
  {
    "objectID": "about.html#about-me",
    "href": "about.html#about-me",
    "title": "Abhijit Darekar",
    "section": "",
    "text": "I have diverse knowledge in Machine Learning and Deep Learning field. I have worked on various projects from showcasing in POC to deploying them in prodution. I have through knwoledge on ML Algorihtm, DL Archiectures, cloud Services such as Azure OpenAI, AWS SageMaker, AWS Lambda Funcions, AWS Api Route, AWS ECR & EC2 and Github Actions for CI/CD.\n      I am active in Kaggel, participating in competations, to testing out LLM(s) and searching for datasets. I have recieved Kaggle Notebooks Exprets Badge.\n\n\n\n\n\n\n\n\n\n\n\nRole\nCompany\nDuration\n\n\n\n\nMachine Learning Engineer\nSkycliff IT\nJan 2024 - Present\n\n\nPython Developer\nTata Consultancy Service\nSep 2020 - Nov 2022"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Abhijit Darekar",
    "section": "",
    "text": "I’m Abhijit Darekar, Machine Learning Engineer by profession with 4 years of expirence in field. Know more about me here.  Currently Iam working in Skycliff It, Bangalore.\n\n\n\nWritings:\nSome of my recent writings are metioned here. Below are some of the recent writings,\n\nNext Word Prediction with Smoothing Technique (Learn more).\nA Gentle Guide to Beam Search with LSTM (Learn more).\n\n\n\nMiscellaneous\n\nI love to travel, weekend bike ride to hill stations nearby in bangalore or long trip to friends (More about rides). I like to read books specially, book realted to personal developemnt, Ficton.\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/beam_search.html",
    "href": "posts/beam_search.html",
    "title": "A Gentle Guide to Beam Search with LSTM",
    "section": "",
    "text": "Image From Internet\n\n\\[\n\\sum_{i=1}^n x_i\n\\]\n\n\n\n Back to top"
  }
]